{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I&E Study 7.1 Automated Stakeholder Analysis for Hedera\n",
    "\n",
    "\n",
    "In this Notebook we will demonstrate how to scrape, clean, analyse and visualise data from different resources to do stakeholder analysis for Hedera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "First we will gather data from the website https://seepnetwork.org. More specifically we will collect the data of the members of the seepnetwork.\n",
    "\n",
    "For this we will define a method that will do the download, or if we already have the file in our local folder we just load it from there to reduce network traffic towards there website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_page(name, path):\n",
    "    \n",
    "    displayname = name.replace('https://', '')\n",
    "    full_path = path+displayname\n",
    "    \n",
    "    content = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(full_path, 'r') as f:\n",
    "            content = f.read().replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    except (OSError, IOError) as e:\n",
    "        response = requests.get(name)\n",
    "        directory = os.path.dirname(full_path)\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "        open(full_path, 'wb').write(response.content)\n",
    "        content = response.content\n",
    "        \n",
    "    return content \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will scrape the main page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"webpages/\"\n",
    "parent_page = \"https://seepnetwork.org\"\n",
    "main_content = load_page(parent_page + \"/Profiles\", file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will get all of the links to the individual member profile pages. Therefor we select all of the elements and extract the href field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "bs = BeautifulSoup(main_content)\n",
    "\n",
    "sub_links = []\n",
    "for link in bs.select(\".mapListViewItem .button.border.blue a\"):\n",
    "    sub_links.append(link[\"href\"])\n",
    "    \n",
    "print(len(sub_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Number of memebers found.\n",
    "\n",
    "To get the members data we download the profile pages and parse the needed fields into an array for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for link in sub_links:\n",
    "    content = load_page(parent_page+link, file_dir)\n",
    "    bs = BeautifulSoup(content)\n",
    "    parts = bs.select(\".sidebar.left\")\n",
    "    \n",
    "    name = parts[0].select_one(\"h3\").string.strip()\n",
    "    years_of_membership = parts[0].select_one(\".sidebarRight > p\").string[0]\n",
    "    location = parts[0].select_one(\".sidebarRight .twoColLeft > p\").contents[-1].strip()\n",
    "    website = parts[0].select_one(\".sidebarRight .twoColRight a\")['href'].strip()\n",
    "    org_type = parts[0].select_one(\".sidebarRight .twoColRight > p\").contents[-1].strip()\n",
    "    mission_statement = parts[1].select(\".sidebarRight > p\")[0].contents[-1].strip()\n",
    "    countries_of_involvement = parts[1].select(\".sidebarRight > p\")[1].contents[-1].strip()\n",
    "    practice_areas = parts[1].select(\".sidebarRight > p\")[2].contents[-1].strip()\n",
    "    \n",
    "    data.append([name, years_of_membership, location, website, org_type, mission_statement, countries_of_involvement, practice_areas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we write the parsed information into a csv file which can later be used as an input for the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "wtr = csv.writer(open ('member_data.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "for member in data : wtr.writerow (member)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
